<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Tao Zhong</title>
    <meta name="description" content="Tao Zhong, Ph.D. student at Princeton University. Research in robotics, computer vision, and machine learning.">
    <meta name="google-site-verification" content="S12lnvgVrU9HMPmoNehLP_uvpvvoOfXDyYeQCCZ0Ofo"/>
    <meta name="author" content="Tao Zhong">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Tao Zhong
                </p>
                <p>
		              I'm a Ph.D. student at <a href="https://www.princeton.edu/">Princeton University</a>, supervised by
                  <a href="https://cablanc.github.io/">Prof. Christine Allen-Blanchette</a>.
                  I received my B.A.Sc. in <a href="https://engsci.utoronto.ca/program/what-is-engsci/">Engineering Science</a>
                  from the University of Toronto, where I worked with <a href="https://animesh.garg.tech/">Prof. Animesh Garg</a>.
                  Previously, I interned at Noah's Ark Lab Canada with <a href="https://users.encs.concordia.ca/~wayang/">Prof. Yang Wang</a> on computer vision and domain generalization,
                  and worked with <a href="https://sse.cuhk.edu.cn/en/faculty/qianhuihuan">Prof. Huihuan Qian</a> at
                  <a href="https://airs.cuhk.edu.cn/en/team">AIRS</a> and CUHK(SZ) on marine robotics.
                </p>
                <p>
                  Feel free to check out my <a href="data/tzhong_cv_2509.pdf">CV</a> or send me an <a href="mailto:tzhong@princeton.edu">Email</a> if you want to connect.
                </p>
                <p style="text-align:center">
                  <a href="mailto:tzhong@princeton.edu">Email</a> &nbsp;/&nbsp;
                  <a href="data/tzhong_cv_2509.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.ca/citations?hl=en&user=0zKusWIAAAAJ">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/neiltaozhong/">LinkedIn</a> &nbsp;/&nbsp;
                  <a href="https://github.com/n3il666">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="images/tzhong.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/tzhong.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Research Interests</h2>
                <p>
                  My research focuses on developing novel machine learning and generative modeling
                  techniques to enable machines to better understand and interact with the physical
                  world. By integrating physical principles and exploiting inherent symmetries, my
                  work aims to create more capable and intelligent autonomous systems for complex,
                  real-world challenges.
                </p>
                <ul>
                  <li><strong>Symmetry and Generalization:</strong> Leveraging symmetries for
                    generalizability and sample efficiency, with applications in robotic manipulation
                    and multi-agent systems.</li>
                  <li><strong>Physics-Guided Generative Models:</strong> Embedding physical principles
                    into generative models to produce physically plausible and diverse solutions for
                    scientific tasks.</li>
                  <li>Some of my early work also focused on domain adaptation and generalization,
                    building models that can robustly adapt to new tasks and domains.</li>
                </ul>
                <!-- <h2>Publications</h2>
                * denotes equal contribution -->
              </td>
            </tr>
          </tbody></table>

          <!-- News -->
          <!-- <table style="width:100%;border:0;border-spacing:0;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>News</h2>
                <ul>
                  <li>Jan 2024 — <strong>GAGrasp</strong> accepted at ICRA 2025.</li>
                  <li>Jan 2024 — <strong>VDPG</strong> accepted at ICLR 2024.</li>
                  <li>Mar 2023 — I will be a Ph.D. student at Princeton University in the incoming fall.</li>
                  <li>Jan 2023 — <strong>Fast-Grasp'D</strong> accepted at ICRA 2023.</li>
                  <li>Sep 2022 — <strong>Meta-DMoE</strong> accepted at NeurIPS 2022.</li>
                </ul>
              </td>
            </tr>
          </tbody></table> -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Publications</h2>
                * denotes equal contribution
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <!-- <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Publications</h2>
                * denotes equal contribution
              </td>
            </tr> -->
    <tr onmouseout="lego_stop()" onmouseover="lego_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='lego_image'><video  width="160" muted autoplay loop>
          <source src="images/lego_after.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/lego_before.png' width="160">
        </div>
        <script type="text/javascript">
          function lego_start() {
            document.getElementById('lego_image').style.opacity = "1";
          }

          function lego_stop() {
            document.getElementById('lego_image').style.opacity = "0";
          }
          lego_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2509.14431">
			<span class="papertitle">Local-Canonicalization Equivariant Graph Neural Networks for Sample-Efficient and Generalizable Swarm Robot Control
</span>
        </a>
        <br>
				<a href="https://www.linkedin.com/in/wang-keqin-1b284618a">Keqin Wang*</a>,
				<strong>Tao Zhong*</strong>,
        <a href="https://www.linkedin.com/in/chang-david">David Chang</a>,
        <a href="https://cablanc.github.io/">Christine Allen-Blanchette</a>
        <br>
        <em>Preprint</em>, 2025
        <br>
        <a href="https://arxiv.org/pdf/2509.14431">arXiv</a>
        <p></p>
        <p>
				We introduce LEGO, a symmetry-aware graph neural network framework that enables sample-efficient, scalable, and generalizable swarm robot control across diverse team sizes and environments.
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <img src='images/g2g.jpg' width="160">
        </div>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2506.02489">
          <span class="papertitle">Grasp2Grasp: Vision-Based Dexterous Grasp Translation via Schrödinger Bridges</span>
        </a>
        <br>
        <strong>Tao Zhong</strong>,
        <a href="https://www.linkedin.com/in/jonah-buchanan-808/">Jonah Buchanan</a>,
        <a href="https://cablanc.github.io/">Christine Allen-Blanchette</a>
        <br>
        <em>NeurIPS</em>, 2025
        <br>
        <a href="https://grasp2grasp.github.io/">project page</a>
        /
        <a href="https://arxiv.org/pdf/2506.02489">arXiv</a>
        <p></p>
        <p>
		Grasp2Grasp enables simulation-free, vision-based translation of dexterous grasps across robot hands using Schrödinger Bridges with physics-informed costs for stable, functionally aligned grasps.
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <img src='images/gagrasp.jpg' width="160">
        </div>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2503.04123">
          <span class="papertitle">GAGrasp: Geometric Algebra Diffusion for Dexterous Grasping</span>
        </a>
        <br>
        <strong>Tao Zhong</strong>,
        <a href="https://cablanc.github.io/">Christine Allen-Blanchette</a>
        <br>
        <em>ICRA</em>, 2025
        <br>
        <a href="https://gagrasp.github.io/">project page</a>
        /
        <a href="https://arxiv.org/pdf/2503.04123">arXiv</a>
        <p></p>
        <p>
		GAGrasp uses a geometric algebra diffusion model to generate robust, physically plausible dexterous grasps that are naturally equivariant to an object's pose.
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <img src='images/vpdg.png' width="160">
        </div>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://chi-chi-zx.github.io/VDPG_ICLR24/">
          <span class="papertitle">Adapting to Distribution Shift by Visual Domain Prompt Generation</span>
        </a>
        <br>
        <a href="https://scholar.google.com/citations?user=0s-HzGIAAAAJ&hl=en">Zhixiang Chi*</a>,
        <a href="https://scholar.google.com/citations?user=crdHC0sAAAAJ&hl=en">Li Gu*</a>,
        <strong>Tao Zhong</strong>,
        <a href="https://www.linkedin.com/in/huan-liu-8094b5137/">Huan Liu</a>,
        <a href="https://scholar.google.com/citations?user=KM4V0a8AAAAJ&hl=en">Yuanhao Yu</a>,
        <a href="https://www.comm.utoronto.ca/~kostas/">Konstantinos N Plataniotis</a>,
        <a href="https://users.encs.concordia.ca/~wayang/">Yang Wang</a>
        <br>
        <em>ICLR</em>, 2024
        <br>
        <a href="https://chi-chi-zx.github.io/VDPG_ICLR24/">project page</a>
        /
        <a href="https://arxiv.org/pdf/2405.02797">arXiv</a>
        <p></p>
        <p>
		We introduce VDPG, a method that adapts large models to new visual domains at test-time using only a few unlabeled images to generate a domain-specific prompt that guides the model's features.
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <img src='images/fastgraspd.png' width="160">
        </div>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://dexgrasp.github.io/">
          <span class="papertitle">Fast-Grasp'D: Dexterous Multi-finger Grasp Generation Through Differentiable Simulation</span>
        </a>
        <br>
        <a href="https://www.cs.toronto.edu/~dylanturpin/">Dylan Turpin</a>,
        <strong>Tao Zhong</strong>,
        <a href="https://tonyzstt.github.io/">Shutong Zhang</a>,
        <a href="https://www.linkedin.com/in/guangleizhu0818">Guanglei Zhu</a>,
        <a href="https://eric-heiden.com/">Eric Heiden</a>,
        <a href="https://blog.mmacklin.com/">Miles Macklin</a>,
        <a href="https://tsogkas.github.io/">Stavros Tsogkas</a>,
        <a href="https://www.cs.toronto.edu/~sven/">Sven Dickinson</a>,
        <a href="https://animesh.garg.tech/">Animesh Garg</a>
        <br>
        <em>ICRA</em>, 2023
        <br>
        <a href="https://dexgrasp.github.io/">project page</a>
        /
        <a href="https://arxiv.org/pdf/2306.08132">arXiv</a>
        <p></p>
        <p>
		    This paper presents Fast-Grasp'D, a differentiable simulator that rapidly generates Grasp'D-1M, a large dataset of stable, contact-rich, multi-finger grasps for robotic learning.
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <img src='images/metadmoe.png' width="160">
        </div>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2210.03885">
          <span class="papertitle">Meta-DMoE: Adapting to Domain Shift by Meta-Distillation from Mixture-of-Experts</span>
        </a>
        <br>
        <strong>Tao Zhong*</strong>,
        <a href="https://scholar.google.com/citations?user=0s-HzGIAAAAJ&hl=en">Zhixiang Chi*</a>,
        <a href="https://scholar.google.com/citations?user=crdHC0sAAAAJ&hl=en">Li Gu*</a>,
        <a href="https://users.encs.concordia.ca/~wayang/">Yang Wang</a>,
        <a href="https://scholar.google.com/citations?user=KM4V0a8AAAAJ&hl=en">Yuanhao Yu</a>,
        <a href="https://dblp.org/pid/56/4951-5.html">Jin Tang</a>
        <br>
        <em>NeurIPS</em>, 2022
        <br>
        <a href="https://arxiv.org/pdf/2210.03885">arXiv</a>
        /
        <a href="https://github.com/n3il666/Meta-DMoE">Code</a>
        <p></p>
        <p>
		We propose Meta-DMoE, a framework that adapts to domain shifts by using a meta-learned aggregator to distill knowledge from specialized expert models into a student network for fast test-time adaptation.
        </p>
      </td>
    </tr>

          </tbody></table>

          <table style="width:100%; margin:0 auto; border:0; border-spacing:0; padding:16px;"><tbody>
            <tr>
              <td>
                <h2>Education</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <img src='images/princeton.png' width="135">
        </div>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <strong>Princeton University</strong>
        <br>
        Ph.D. in Mechanical and Aerospace Engineering (Robotics Track)
        <br>
        2023.09 - Present
        <p></p>
        cGPA: 4.0
      </td>
    </tr>

    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <img src='images/uoft.png' width="135">
        </div>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <strong>University of Toronto</strong>
        <br>
        B.A.Sc. in Engineering Science with High Honours
        <br>
        Major in Robotics Engineering, Minor in Artificial Intelligence
        <br>
        2018.09 - 2023.06
        <p></p>
        cGPA: 3.81
      </td>
    </tr>

          </tbody></table>


					<table style="width:100%; margin:0 auto; border:0; border-spacing:0; padding:16px;"><tbody>
            <tr>
              <td>
                <h2>Miscellanea</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


            <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #aaba9e;">
								 <h2>Academic Service</h2>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:center">
                Reviewer, <a href="https://neurips.cc/">NeurIPS</a> (2025),
                <a href="https://www.ieee-ras.org/publications/ra-l">RA-L</a>,
                <a href="https://iclr.cc/">ICLR</a> (2025, 2026),
                <a href="https://l4dc.web.ox.ac.uk/home">L4DC</a> (2024)
              </td>
            </tr>

            <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #c6b89e;">
								 <h2>Teaching</h2>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:center">
                Teaching Assistant, <a href="https://registrar.princeton.edu/course-offerings/course-details?term=1262&courseid=002363">MAE433 Fall 2025</a>, Princeton University
              </td>
            </tr>
			
          </tbody></table>
        </td>
      </tr>
    </table>

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:center;font-size:small;">
                  Website source code adapted from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>'s template.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>

    <!-- Invisible ClustrMaps hit tracker (hidden but still loads) -->
    <img
    src="https://www.clustrmaps.com/map_v2.png?d=HUn2Q3BLfPJ6obZqwF3VxgrxX2MRfhZOo4JjsepP1a4&cl=ffffff"
    width="1" height="1"
    />
  </body>
</html>
